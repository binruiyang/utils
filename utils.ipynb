{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import optuna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from lightgbm.callback import LightGBMPruningCallback\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "def objective(trial, X, y): \n",
    "    \n",
    "    param_grid = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.001, 0.1),\n",
    "        \"num_iterations\": trial.suggest_int(\"num_iterations\", 100, 1000),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 10, 100),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 100, 1000),\n",
    "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.1, 1, step = 0.1),\n",
    "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 10),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.1, 1, step = 0.1), \n",
    "        'scale_pos_weight': trial.suggest_float('scale_pos_weight', 1, 9, step = 0.5),\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 0, 10, step = 0.1),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 0, 10, step = 0.1),\n",
    "        'verbose': -1\n",
    "    }\n",
    "    \n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=5, random_state=42)\n",
    "    \n",
    "    cv_scores = []\n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, y)):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        model = LGBMClassifier(objective=\"binary\", **param_grid)\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=[(X_test, y_test)],\n",
    "            eval_metric=\"auc\",\n",
    "            early_stopping_rounds=500,\n",
    "            callbacks=[\n",
    "                LightGBMPruningCallback(trial, \"auc\")\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        pred = model.predict_proba(X_test)[:, 1]\n",
    "        cv_scores.append(roc_auc_score(y_test, pred))\n",
    "\n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(lambda trial: objective(trial, X, y), n_trials=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Define the custom F-1 score function\n",
    "def custom_f1(y_pred, dtrain):\n",
    "    y_true = dtrain.get_label()\n",
    "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "    return 'f1', f1_score(y_true, y_pred_binary)\n",
    "\n",
    "# Define the Optuna objective function\n",
    "def objective(trial):\n",
    "    # Define the hyperparameter search space\n",
    "    params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'min_child_weight': trial.suggest_loguniform('min_child_weight', 1e-3, 1e1),\n",
    "        'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
    "        'gamma': trial.suggest_loguniform('gamma', 1e-3, 1e1),\n",
    "    }\n",
    "\n",
    "    # Load your data\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "\n",
    "    # Perform cross-validation\n",
    "    cv_results = xgb.cv(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=trial.suggest_int('num_boost_round', 50, 500),\n",
    "        nfold=5,\n",
    "        feval=custom_f1,\n",
    "        maximize=True,\n",
    "        early_stopping_rounds=10,\n",
    "        verbose_eval=False,\n",
    "    )\n",
    "\n",
    "    # Get the best F-1 score\n",
    "    best_f1 = np.max(cv_results['test-f1-mean'])\n",
    "    return best_f1\n",
    "\n",
    "# Create and run the Optuna study\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Print the best parameters\n",
    "print('Best hyperparameters: ', study.best_params)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
