{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5vtG0bbJt9u"
   },
   "source": [
    "# **GR5242 HW04 Problem 1: Shakespeare with LSTM networks**\n",
    "\n",
    "**Instructions**: This problem is an individual assignment -- you are to complete this problem on your own, without conferring with your classmates.  You should submit a completed and published notebook to Courseworks; no other files will be accepted.\n",
    "\n",
    "## Description:\n",
    "\n",
    "This homework exercise has 3 primary goals:\n",
    " * Introduce some basic concepts from natural language processing\n",
    " * Get some practice training recurrent neural networks, specifically on text data\n",
    " * Be able to generate fake text data from your favorite author!   \n",
    "\n",
    "By the end of this exercise, you will have a basic, but decent, computer program which can simulate the writing patterns of any author of your choice.\n",
    "\n",
    "Here is an outline of the rest of the exercise.\n",
    " 1. Data loading\n",
    "     - We will start by downloading a text from Project Gutenberg that we will try to model\n",
    "     - Data preprocessing and numerical encoding\n",
    "     - Making training `Dataset` and `DataLoader` objects\n",
    " 3. Learn to generate text with a neural network\n",
    "     - Defining the recurrent network\n",
    "     - Training\n",
    "     - Predicting and sampling text from the model\n",
    "\n",
    "     There are 12 questions (70 points) in total, which include coding and written questions. You can only modify the codes and text within \\### YOUR CODE HERE ### and/or \\### YOUR ANSWER HERE ###.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "6hwabXFdkMlH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_-QkOsDQrgu"
   },
   "source": [
    "## Character-level language modeling\n",
    "\n",
    "Our goal here is to build a model of language letter-by-letter. Since we may also allow numbers, spaces, and punctuation, it's better to say character-by-character. We will start by fixing an \"alphabet\": the set of allowed characters.\n",
    "\n",
    "In math notation, let's call the alphabet $A$. In code,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "K8RdHuHPRCPJ"
   },
   "outputs": [],
   "source": [
    "alphabet = \" \"\"'abcdefghijklmnopqrstuvwxyz1234567890.,!?:;ABCDEFGHIJKLMNOPQRSTUVWXYZ\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sitlEu4BJsBm"
   },
   "source": [
    "# Section 1: Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oryq1WYLNojq"
   },
   "source": [
    "We will start by downloading training data from Project Gutenberg: https://www.gutenberg.org/. Project Gutenberg is a free repository of public domain books. Find any book you like, and download it in Plain Text UTF-8 format.\n",
    "\n",
    "For example, we will use Shakespeare's complete works: https://www.gutenberg.org/ebooks/100. There is a link on that page to the Plain Text format data.  Download the pg100.txt file, and then upload it from your computer to colab (click at left on the File icon, then click the upload icon).  \n",
    "\n",
    "*Important*: whichever work you choose, make sure you have enough data! The size of your plain text file should be at least 2MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NeGF0qSbJqqB",
    "outputId": "b4bcb981-dd3d-41d4-beb6-ac5058cc0a32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Project Gutenberg eBook of Moby Dick; Or, The Whale\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and with almost no restrictions\n",
      "whatsoever. You may copy it, give it away or re-use it under the terms\n",
      "of the Project Gutenberg License included with this ebook or online\n",
      "at www.gutenberg.org. If you are not located in the United States,\n",
      "you will have to check the laws of the country where you are located\n",
      "before using this eBook.\n",
      "\n",
      "Title: Moby Dick; Or, The Whale\n",
      "\n",
      "Author: Herman Melville\n",
      "\n",
      "Release date: July 1, 2001 [eBook #2701]\n",
      "                Most recently updated: August 18, 2021\n",
      "\n",
      "Language: English\n",
      "\n",
      "Credits: Daniel Lazarus, Jonesey, and David Widger\n",
      "\n",
      "\n",
      "*** START OF THE PROJECT GUTENBERG EBOOK MOBY DICK; OR, THE WHALE ***\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "MOBY-DICK;\n",
      "\n",
      "or, THE WHALE.\n",
      "\n",
      "By Herman Melville\n",
      "\n",
      "\n",
      "\n",
      "CONTENTS\n",
      "\n",
      "ETYMOLOGY.\n",
      "\n",
      "EXTRACTS (Supplied by a Sub-Sub-Librarian).\n",
      "\n",
      "CHAPTER 1. Loomings.\n",
      "\n",
      "CHAPTER 2. \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# URL of the book's plain text file on Project Gutenberg\n",
    "url = \"https://www.gutenberg.org/cache/epub/2701/pg2701.txt\"  # Moby Dick; Or, The Whale\n",
    "\n",
    "# Fetch and read the book\n",
    "response = requests.get(url)\n",
    "text = response.text\n",
    "\n",
    "# Display the first 1000 characters to verify\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GGNYDQKRSXBk"
   },
   "source": [
    "Let's load the text and see what it says:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7RKK_A1NShRW",
    "outputId": "032688ce-4c2f-403e-ea2e-c3e41054b57b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text is 1260542 characters long.\n",
      "\n",
      "A sample from the middle:\n",
      "\n",
      "whole boat in its complicated coils,\n",
      "twisting and writhing around it in almost every direction. All\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"text is\", len(text), \"characters long.\")\n",
    "print()\n",
    "print(\"A sample from the middle:\")\n",
    "print()\n",
    "print(text[len(text) // 2 : len(text) // 2 + 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7viKRe94P_Cx"
   },
   "source": [
    "### Data standardization\n",
    "\n",
    "Now, we will clean the data: converting the data to lowercase, removing extra spaces and linebreaks, and get rid of characters which are not in our alphabet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "5m3ZqU7RS2xA"
   },
   "outputs": [],
   "source": [
    "# remove extra characters by replacing them with spaces\n",
    "text = re.sub(rf\"[^{alphabet}]\", \" \", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7tgMpHS2O67G"
   },
   "source": [
    "Let's see how it looks again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TiJEL-O-O6wL",
    "outputId": "5e40729d-31c0-44c4-fe48-283735111f38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But high above the flying scud and dark rolling clouds, there \n",
      "floated a little isle of sunlight, from which beamed forth an angel \n"
     ]
    }
   ],
   "source": [
    "a = 110042\n",
    "b = a+131\n",
    "x_prompt = text[a:b]\n",
    "print(x_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8JFrb5Jr92F8"
   },
   "source": [
    "### Numerical encoding\n",
    "\n",
    "Unfortunately, neural networks don't understand text. So, we need to convert our characters to numerical values. Here are some helper functions for doing this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "1LjvsaNy91to"
   },
   "outputs": [],
   "source": [
    "# let's build a dictionary mapping characters to integers\n",
    "char2int = {c: i for i, c in enumerate(alphabet)}\n",
    "alphabet_array = np.array([c for c in alphabet])\n",
    "\n",
    "# this function will turn a string into a numpy array of integers\n",
    "def int_encode(string):\n",
    "  if any(c not in char2int for c in string):\n",
    "    raise ValueError(\n",
    "        \"Found a character which was not in the alphabet in the input \"\n",
    "        f\"to int_encode. Valid alphabet characters: {alphabet}\"\n",
    "      )\n",
    "  return np.array([char2int[c] for c in string])\n",
    "\n",
    "# this function will decode a numpy array of integers back to a string\n",
    "def int_decode(int_array):\n",
    "  return ''.join(alphabet_array[int_array])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LsSPJalSWFUm"
   },
   "source": [
    "(Question 1a: 4 points) Test out `int_encode` by passing `test_string` in and printing the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "vU8Umqsw-4CN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[45 22 21  0  9 10  8  9  0  2  3 16 23  6  0 21  9  6  0  7 13 26 10 15\n",
      "  8  0 20  4 22  5  0  2 15  5  0  5  2 19 12  0 19 16 13 13 10 15  8  0\n",
      "  4 13 16 22  5 20 39  0 21  9  6 19  6  0 70  7 13 16  2 21  6  5  0  2\n",
      "  0 13 10 21 21 13  6  0 10 20 13  6  0 16  7  0 20 22 15 13 10  8  9 21\n",
      " 39  0  7 19 16 14  0 24  9 10  4  9  0  3  6  2 14  6  5  0  7 16 19 21\n",
      "  9  0  2 15  0  2 15  8  6 13  0]\n"
     ]
    }
   ],
   "source": [
    "# Let's test these out!\n",
    "### YOUR CODE HERE ###\n",
    "print(int_encode(x_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wGFYXZPcWL8Z"
   },
   "source": [
    "(Question 1b: 4 points) Decode the result from the last cell using `int_decode` to make sure it is the same as `test_string`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "392dWPKn0J85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But high above the flying scud and dark rolling clouds, there \n",
      "floated a little isle of sunlight, from which beamed forth an angel \n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE ###\n",
    "print(int_decode(int_encode(x_prompt)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SWxJykne_VKY"
   },
   "source": [
    "Is the decoding the same as `test_string`? It should -- you have a bug above if not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oIqRJLtyAC--"
   },
   "source": [
    "### Make a training dataset\n",
    "\n",
    "First, we make a numerical encoded version of the entire dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "JOr_T3ddALvQ"
   },
   "outputs": [],
   "source": [
    "enctext = int_encode(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1FLc9MABXKu"
   },
   "source": [
    "Use `torch.tensor` to make it into a PyTorch tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ORDJL4_0BWTK",
    "outputId": "6dea873a-ea3b-44e1-95dd-a58d653df8a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0, 63,  9,  ..., 70,  0, 70])\n"
     ]
    }
   ],
   "source": [
    "enctext = torch.tensor(enctext)\n",
    "print(enctext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pk3z0bACJyQc"
   },
   "source": [
    "# Section 2: Training a NN\n",
    "\n",
    "Our model will work as follows:\n",
    " - One-hot encoded input gets passed into a linear embedding layer. These two operations are combined with the `Embedding` layer: https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
    " - LSTM cell\n",
    " - Linear decoder layer\n",
    "\n",
    "Torch has two main ways of interfacing with recurrent networks. In the case of LSTMs, those are:\n",
    " - the LSTM layer https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM\n",
    " - the LSTMCell layer https://pytorch.org/docs/stable/generated/torch.nn.LSTMCell.html\n",
    "\n",
    "Both models are sequential: the goal is to process a batch of sequences of input features and produce a batch of sequences of output features. The `LSTM` class makes this simple and easy, and the `LSTMCell` class gives more control by allowing you to process the sequences one element at a time. We will use the `LSTM` layer to keep things simple, but keep in mind that some of what we do could be made more efficient with `LSTMCell`.\n",
    "\n",
    "The inputs and outputs to recurrent networks in Torch have shape: `(batch_dimension, sequence_dimension, feature_dimension)`. In this case, our feature dimension is `len(alphabet)`.\n",
    "\n",
    "Something to keep in mind: the output of this network will be stateful! In each batch, the `k`th output along the sequence dimension will be the logits for predicting the `k+1`th input in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "F80hsi-FH4WA"
   },
   "outputs": [],
   "source": [
    "# We will use this constant below\n",
    "HIDDEN_DIM = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "_ZUt-1--LP32"
   },
   "outputs": [],
   "source": [
    "# Defining some parameters about data batching, explained in the next section\n",
    "# Note: after you get the entire assignment working, you can make these\n",
    "# bigger and train for longer, to get better performance\n",
    "SEQUENCE_LENGTH = 128\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h2ORoPzNJkKP"
   },
   "source": [
    "### Making the dataset of (input, target) pairs\n",
    "\n",
    "To train the model, we need to make a `torch.utils.data.Dataset` containing input and target sequences. Our input sequences will be sequences of length `SEQUENCE_LENGTH` containing int-encoded characters from the input. Our target sequences will be the \"next characters\" corresponding to the input sequence: so, if the input sequence is the 10th, 11th, ... characters, then the target sequence is the 11th, 12th, ... characters.\n",
    "\n",
    "We will walk through using `torch.utils.data.Dataset` methods to create these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7PlSDJOjL_Fb"
   },
   "source": [
    "(Question 2c: 8 points) Write a `batch` function for a `torch` tensor, which we defined above, to make disjoint consecutive sequences of consecutive characters of length `SEQUENCE_LENGTH`. `torch.split()` and `torch.vstack()` may be useful. Remember to be careful of the edge case that arises when `len(enctext) % SEQUENCE_LENGTH != 0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "dX6fJc84W7Lo"
   },
   "outputs": [],
   "source": [
    "def batch(enctext, SEQUENCE_LENGTH):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    num_complete_seqs = len(enctext) // SEQUENCE_LENGTH\n",
    "    \n",
    "    usable_length = num_complete_seqs * SEQUENCE_LENGTH\n",
    "    text_subset = enctext[:usable_length]\n",
    "    batched = text_subset.view(-1, SEQUENCE_LENGTH)\n",
    "    \n",
    "    return batched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n24Q5ySjU2ed"
   },
   "source": [
    "(Question 2d: 8 points) Now, use batch to create target sequences from the following version of the dataset which has been offset by 1 element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "BkjMZfS3XAae"
   },
   "outputs": [],
   "source": [
    "input_seqs  = batch(enctext, SEQUENCE_LENGTH)\n",
    "target_seqs = batch(enctext[1:], SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geD7pQ7TVH1Z"
   },
   "source": [
    "(Question 2e: 6 points) Now, use the `torch` builtin class `torch.utils.data.TensorDataset` to create a dataset of (input, target) pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B5rrw7OLXGjS",
    "outputId": "4662f219-ad1a-4e27-8a1b-a97d0b61d773"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.TensorDataset at 0x7f8cf031aa60>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = TensorDataset(input_seqs, target_seqs)\n",
    "pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JX1_4b0oVtdv"
   },
   "source": [
    "(Question 2f: 4 points) Finally, define a `torch.utils.data.DataLoader` object to generate batches of pairs of length `BATCH_SIZE`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pE547bYpXJge",
    "outputId": "493d534a-ed6d-4067-cc77-f47b942ce0e7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f8cf031d910>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = DataLoader(pairs, batch_size=BATCH_SIZE, shuffle=True)\n",
    "train_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VcgjwlrDMOdz"
   },
   "source": [
    "You may uncomment the below cell if you would like to understand the structure of the `train_loader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "d5q07j73MOdz"
   },
   "outputs": [],
   "source": [
    "# for i, (x, y) in enumerate(train_loader):\n",
    "#    print(x, y)\n",
    "#    if i > 1:\n",
    "#        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2syDhfkWdXt"
   },
   "source": [
    "(Question 2a: 10 points) Model definition: make a Sequential model with an Embedding layer with input dimension `len(alphabet)` and output dimension `HIDDEN_DIM`, followed by an LSTM layer with `HIDDEN_DIM` features, followed by a Linear layer with `len(alphabet)` features. A helper class is provided to extract tensors from the output of the LSTM layer to prepare as input to the input of the final linear layer. Use of this class in the Sequential container would look something like `('extract', extract_tensor(return_sequences=return_sequences))`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "REnQC0MXiyAL"
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "return_sequences = True\n",
    "\n",
    "# LSTM() returns tuple of (tensor, (recurrent state))\n",
    "class extract_tensor(nn.Module):\n",
    "    def __init__(self, return_sequences=False):\n",
    "        super(extract_tensor, self).__init__()\n",
    "        self.return_sequences = return_sequences\n",
    "\n",
    "    def forward(self,x):\n",
    "        # Output shape (batch, features, hidden)\n",
    "        tensor, _ = x\n",
    "        # Reshape shape (batch, hidden)\n",
    "        if not self.return_sequences:\n",
    "            tensor = tensor[:, -1, :]\n",
    "        return tensor\n",
    "\n",
    "model = nn.Sequential(OrderedDict([\n",
    "    ('embed', nn.Embedding(len(alphabet), HIDDEN_DIM)),\n",
    "    ('lstm', nn.LSTM(HIDDEN_DIM, HIDDEN_DIM, batch_first=True)),\n",
    "    ('extract', extract_tensor(return_sequences=return_sequences)),\n",
    "    ('decode', nn.Linear(HIDDEN_DIM, len(alphabet)))\n",
    "]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fzp4Q7vFJXVj"
   },
   "source": [
    "(Question 2b: 8 points) If we want to use the output of the model as logits for predicting a character (which we can think of as a class), what loss should we use? Name this `criterion`. Additionally, define an optimizer to use in training. As per usual, we will recommend the use of `optim.Adam`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "u1XFATDzieB5"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Psw5Vbpk3J3Q"
   },
   "source": [
    "(Question 2g: 2 points) Train the model!\n",
    "\n",
    "\n",
    "Write a training loop in PyTorch for a model that processes batched input data. Use `NUM_EPOCHS = 40` as the number of training epochs, and ensure that:\n",
    "1. Each epoch consists of iterating over batches from a `train_loader`.\n",
    "2. For each batch, the model's gradients are zeroed, a forward pass is made, and a loss is calculated using a provided criterion.\n",
    "3. After each batch's loss is calculated, perform backpropagation and optimizer steps.\n",
    "4. Track and print the average loss at the end of each epoch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "wrOmFS_eV21G"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/40], Average Loss: 1.8243\n",
      "Epoch [10/40], Average Loss: 1.6756\n",
      "Epoch [15/40], Average Loss: 1.6017\n",
      "Epoch [20/40], Average Loss: 1.5555\n",
      "Epoch [25/40], Average Loss: 1.5239\n",
      "Epoch [30/40], Average Loss: 1.5000\n",
      "Epoch [35/40], Average Loss: 1.4821\n",
      "Epoch [40/40], Average Loss: 1.4672\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 40\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch_inputs, batch_targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        outputs = model(batch_inputs)\n",
    "        outputs = outputs.view(-1, len(alphabet))\n",
    "        batch_targets = batch_targets.view(-1)\n",
    "        \n",
    "        loss = criterion(outputs, batch_targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    if (epoch + 1) % 5 == 0: \n",
    "        print(f'Epoch [{epoch+1}/{NUM_EPOCHS}], Average Loss: {avg_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1WbYcgKmj1zL"
   },
   "source": [
    "Here, make sure the loss goes down as it trains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rmysazCNc14W"
   },
   "source": [
    "# Section 3: Did it work? Let's see what the model learned\n",
    "\n",
    "Here, we'll write some functions to see how well the model has learned to predict text and to draw samples from the model.\n",
    "\n",
    "First, we'll give you a function to \"seed\" the model with some input text and then predict the most likely future text. It will be your job to create a variation on this function in the question below, so make sure you understand how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "UUaB5I6VXTdi"
   },
   "outputs": [],
   "source": [
    "def predict(seed_string, sample_length=50):\n",
    "  # Convert seed_string to int\n",
    "  current_text_ints = list(int_encode(seed_string))\n",
    "\n",
    "  for i in range(sample_length):\n",
    "    # Add an empty batch dimension and convert to tensor\n",
    "    text_arr = np.array(current_text_ints).reshape(1, -1)\n",
    "    text_arr = torch.tensor(text_arr)\n",
    "\n",
    "    # set our model to return only one output instead of the sequence\n",
    "    model.extract.return_sequences = False\n",
    "\n",
    "    # Get the full sequence of predictions, remove the batch dim\n",
    "    logits = model(text_arr)\n",
    "\n",
    "    # Remove the batch dimension and get the final logits\n",
    "    final_logits = logits[-1]\n",
    "\n",
    "    # Get the prediction using tf.argmax\n",
    "    pred = torch.argmax(final_logits)\n",
    "\n",
    "    # Append this to `current_text_ints`\n",
    "    current_text_ints.append(pred.numpy())\n",
    "\n",
    "  return int_decode(np.array(current_text_ints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "90ht1OAtegjv",
    "outputId": "a48694b3-4280-4b67-ac3d-7e7ae685734a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial prompt (x_prompt) used for prediction:\n",
      "But high above the flying scud and dark rolling clouds, there \n",
      "floated a little isle of sunlight, from which beamed forth an angel \n",
      "\n",
      "Extracted substring (x_prompt_plus) from 'text' starting at index 110042 up to index 110323 :\n",
      "But high above the flying scud and dark rolling clouds, there \n",
      "floated a little isle of sunlight, from which beamed forth an angel s \n",
      "face; and this bright face shed a distinct spot of radiance upon the \n",
      "ship s tossed deck, something like that silver plate now inserted into \n",
      "the V\n",
      "\n",
      "Predicted text based on x_prompt with a prediction length of 150 characters:\n",
      "But high above the flying scud and dark rolling clouds, there \n",
      "floated a little isle of sunlight, from which beamed forth an angel and the ship s \n",
      "content to the ship s and strange the ship s \n",
      "content to the ship s and strange the ship s \n",
      "content to the ship s and strange the ship\n"
     ]
    }
   ],
   "source": [
    "pred_length = 150\n",
    "\n",
    "# Print the initial prompt (x_prompt) used for prediction\n",
    "print(\"Initial prompt (x_prompt) used for prediction:\")\n",
    "print(x_prompt)  # Assuming x_prompt is already defined\n",
    "\n",
    "a = 110042\n",
    "bb = a + 131 + pred_length\n",
    "x_prompt_plus = text[a:bb]  # Extracting substring from text\n",
    "\n",
    "# Print the extracted substring from 'text' within the specified range\n",
    "print(\"\\nExtracted substring (x_prompt_plus) from 'text' starting at index\", a, \"up to index\", bb, \":\")\n",
    "print(x_prompt_plus)\n",
    "\n",
    "# Print the predicted text based on x_prompt with specified prediction length\n",
    "print(\"\\nPredicted text based on x_prompt with a prediction length of\", pred_length, \"characters:\")\n",
    "print(predict(x_prompt, pred_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1gT8Oi_KgocR"
   },
   "outputs": [],
   "source": [
    "# feel free to try your own seed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5f92OC-gORk"
   },
   "source": [
    "It seems like maybe the model learned something, but the output is a little boring. Let's make it more interesting with *randomness*!\n",
    "\n",
    "Right now, the function always picks the most likely next letter. Instead, let's sample the next letter from the model's predicted probability distribution.\n",
    "\n",
    "(Question 3a: 8 points) Fill in the blanks in the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "m4Oo888KiQyS"
   },
   "outputs": [],
   "source": [
    "def generate(seed_string, sample_length=50, temperature=0.7):\n",
    "    current_text_ints = list(int_encode(seed_string))\n",
    "\n",
    "    for i in range(sample_length):\n",
    "        text_arr = np.array(current_text_ints).reshape(1, -1)\n",
    "        text_arr = torch.tensor(text_arr)\n",
    "\n",
    "        model.extract.return_sequences = False\n",
    "        logits = model(text_arr)\n",
    "        final_logits = logits[-1]\n",
    "\n",
    "        # Apply temperature scaling\n",
    "        logits_temp = final_logits / temperature\n",
    "        probs = F.softmax(logits_temp, dim=0)\n",
    "        probs = probs.detach().numpy()\n",
    "\n",
    "        # Sample from the distribution\n",
    "        sample = np.random.choice(len(alphabet), p=probs)\n",
    "        current_text_ints.append(sample)\n",
    "\n",
    "    return int_decode(np.array(current_text_ints))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kp07ZVlUhtpy"
   },
   "source": [
    "(Question 3b: 6 points) Test this function `generate`. Is its output different from `predict`? How does it differ, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d75C0gswiOU2",
    "outputId": "84ea4204-cb22-42a7-ea41-b504b8b158f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial prompt (x_prompt) used for prediction:\n",
      "But high above the flying scud and dark rolling clouds, there \n",
      "floated a little isle of sunlight, from which beamed forth an angel \n",
      "\n",
      "Extracted substring (x_prompt_plus) from 'text' starting at index 110042 up to index 110323 :\n",
      "But high above the flying scud and dark rolling clouds, there \n",
      "floated a little isle of sunlight, from which beamed forth an angel s \n",
      "face; and this bright face shed a distinct spot of radiance upon the \n",
      "ship s tossed deck, something like that silver plate now inserted into \n",
      "the V\n",
      "\n",
      "Predicted text based on x_prompt with a prediction length of 150 characters:\n",
      "But high above the flying scud and dark rolling clouds, there \n",
      "floated a little isle of sunlight, from which beamed forth an angel and the ship s \n",
      "content to the ship s and strange the ship s \n",
      "content to the ship s and strange the ship s \n",
      "content to the ship s and strange the ship\n",
      "\n",
      "Generated text based on x_prompt with a prediction length of 150 characters:\n",
      "But high above the flying scud and dark rolling clouds, there \n",
      "floated a little isle of sunlight, from which beamed forth an angel and this wholly leaded in the dults to his such the crowds of the ship? I ll Captain Ahab s attending in actional in \n",
      "the mariners of the landlord you\n",
      "\n",
      "Generated text based on x_prompt with a prediction length of 150 characters:\n",
      "But high above the flying scud and dark rolling clouds, there \n",
      "floated a little isle of sunlight, from which beamed forth an angel to the captain seemed them \n",
      "to pinal in sucelliate hold of those is of the prencher s sailors and comparated dead \n",
      "comporant in his order help the gre\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# Print the initial prompt (x_prompt) used for prediction\n",
    "print(\"Initial prompt (x_prompt) used for prediction:\")\n",
    "print(x_prompt)  # Assuming x_prompt is already defined\n",
    "\n",
    "a = 110042\n",
    "bb = a + 131 + pred_length\n",
    "x_prompt_plus = text[a:bb]  # Extracting substring from text\n",
    "\n",
    "# Print the extracted substring from 'text' within the specified range\n",
    "print(\"\\nExtracted substring (x_prompt_plus) from 'text' starting at index\", a, \"up to index\", bb, \":\")\n",
    "print(x_prompt_plus)\n",
    "\n",
    "# Print the predicted text based on x_prompt with specified prediction length\n",
    "print(\"\\nPredicted text based on x_prompt with a prediction length of\", pred_length, \"characters:\")\n",
    "print(predict(x_prompt, pred_length))\n",
    "\n",
    "# Print the predicted text based on x_prompt with specified prediction length\n",
    "print(\"\\nGenerated text based on x_prompt with a prediction length of\", pred_length, \"characters:\")\n",
    "print(generate(x_prompt, pred_length))\n",
    "\n",
    "\n",
    "# Print the predicted text based on x_prompt with specified prediction length\n",
    "print(\"\\nGenerated text based on x_prompt with a prediction length of\", pred_length, \"characters:\")\n",
    "print(generate(x_prompt, pred_length))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fy3J9i71hnWT"
   },
   "source": [
    "(Question 3c: 2 point) Try running `generate` a few times. Are the results the same or different? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated text based on x_prompt with a prediction length of 150 characters:\n",
      "But high above the flying scud and dark rolling clouds, there \n",
      "floated a little isle of sunlight, from which beamed forth an angel be seem them in his and being and with the strongly and containing over the near the sea of the fear were in its round your \n",
      "no contain peared in the \n",
      "\n",
      "Generated text based on x_prompt with a prediction length of 150 characters:\n",
      "But high above the flying scud and dark rolling clouds, there \n",
      "floated a little isle of sunlight, from which beamed forth an angel \n",
      "suddenly a \n",
      "same every sezzled a hundred with age at one of the silentering remains, and sailors, in one and some man of the whale s way on the grand\n",
      "\n",
      "Generated text based on x_prompt with a prediction length of 150 characters:\n",
      "But high above the flying scud and dark rolling clouds, there \n",
      "floated a little isle of sunlight, from which beamed forth an angel to the shee \n",
      "whales, but seemed transped you see him and renom can substended by never and the fish had made of the whale in the ship, in his flashed \n"
     ]
    }
   ],
   "source": [
    "# Print the predicted text based on x_prompt with specified prediction length\n",
    "print(\"\\nGenerated text based on x_prompt with a prediction length of\", pred_length, \"characters:\")\n",
    "print(generate(x_prompt, pred_length))\n",
    "\n",
    "\n",
    "# Print the predicted text based on x_prompt with specified prediction length\n",
    "print(\"\\nGenerated text based on x_prompt with a prediction length of\", pred_length, \"characters:\")\n",
    "print(generate(x_prompt, pred_length))\n",
    "\n",
    "\n",
    "\n",
    "# Print the predicted text based on x_prompt with specified prediction length\n",
    "print(\"\\nGenerated text based on x_prompt with a prediction length of\", pred_length, \"characters:\")\n",
    "print(generate(x_prompt, pred_length))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output will be different each time you run the generate function because it's sampling from a probability distribution rather than always taking the most likely next character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
