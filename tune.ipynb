{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from optuna import create_study, logging\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.integration import XGBoostPruningCallback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "def objective(trial, X, y, group, score, params):\n",
    "    dtrain = xgb.DMatrix(X, label=y)\n",
    "    \n",
    "    ## Initial Learning Parameters\n",
    "    params['learning_rate'] = 0.1\n",
    "    params['num_boost_round'] = 1000\n",
    "\n",
    "    if group == '1':\n",
    "        params['max_depth'] = trial.suggest_int('max_depth', 2, 10)\n",
    "        params['min_child_weight'] = trial.suggest_loguniform('min_child_weight',\n",
    "                                                              1e-10, 1e10)\n",
    "    \n",
    "    if group == '2':\n",
    "        params['subsample'] = trial.suggest_uniform('subsample', 0, 1)\n",
    "        params['colsample_bytree'] = trial.suggest_uniform('colsample_bytree', 0, 1)\n",
    "        params['gamma'] = trial.suggest_float('gamma', 0, 3)\n",
    "    \n",
    "    if group == '3':\n",
    "        params['learning_rate'] = trial.suggest_uniform('learning_rate', 0, 0.1)\n",
    "        params['num_boost_round'] = trial.suggest_int('num_boost_round', 100, 1000)\n",
    "\n",
    "    pruning_callback = XGBoostPruningCallback(trial, \"test-\" + score.__name__)\n",
    "    cv_scores = xgb.cv(params, dtrain, nfold=5,\n",
    "                       stratified=True,\n",
    "                       feval=score,\n",
    "                       early_stopping_rounds=10,\n",
    "                       callbacks=[pruning_callback],\n",
    "                       seed=0)\n",
    "\n",
    "    return cv_scores['test-' + score.__name__ + '-mean'].values[-1]\n",
    "\n",
    "\n",
    "def execute_optimization(study_name, group, score, trials,\n",
    "                         params=dict(), direction='maximize'):\n",
    "    logging.set_verbosity(logging.ERROR)\n",
    "    \n",
    "    ## We use pruner to skip trials that are NOT fruitful\n",
    "    pruner = MedianPruner(n_warmup_steps=5)\n",
    "    \n",
    "    study = create_study(direction=direction,\n",
    "                         study_name=study_name,\n",
    "                         storage='sqlite:///optuna.db',\n",
    "                         load_if_exists=True,\n",
    "                         pruner=pruner)\n",
    "\n",
    "    study.optimize(lambda trial: objective(trial, X_train, y_train,\n",
    "                                           group, score, params),\n",
    "                   n_trials=trials,\n",
    "                   n_jobs=-1)\n",
    "    \n",
    "    \n",
    "    print(\"STUDY NAME: \", study_name)\n",
    "    print('------------------------------------------------')\n",
    "    print(\"EVALUATION METRIC: \", score.__name__)\n",
    "    print('------------------------------------------------')\n",
    "    print(\"BEST CV SCORE\", study.best_value)\n",
    "    print('------------------------------------------------')\n",
    "    print(f\"OPTIMAL GROUP - {group} PARAMS: \", study.best_params)\n",
    "    print('------------------------------------------------')\n",
    "    print(\"BEST TRIAL\", study.best_trial)\n",
    "    print('------------------------------------------------')\n",
    "    \n",
    "    \n",
    "    return study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_func = metrics.f1_score\n",
    "def score_function(y_pred, dtrain):\n",
    "    y_pred = (y_pred > 0.5).astype(int)\n",
    "    y_true = (dtrain.get_label() > 0.5).astype(int)\n",
    "    return score_func.__name__, score_func(y_true, y_pred)\n",
    "\n",
    "score_function.__name__ = score_func.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stepwise_optimization(trials=10):\n",
    "    final_params = dict()\n",
    "    for g in ['1', '2', '3']:\n",
    "        print(f\"=========================== Optimizing Group - {g} ============================\")\n",
    "        update_params = execute_optimization('xgboost', g, score_function, trials,\n",
    "                                             params=final_params, direction='maximize')\n",
    "        final_params.update(update_params)\n",
    "        print(f\"PARAMS after optimizing GROUP - {g}: \", final_params)\n",
    "        print()\n",
    "        print()\n",
    "\n",
    "    print(\"=========================== FINAL OPTIMAL PARAMETERS ============================\")\n",
    "    print(final_params)\n",
    "    \n",
    "    return final_params\n",
    "\n",
    "\n",
    "params = stepwise_optimization(200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
